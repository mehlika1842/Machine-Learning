{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing the libraries"
      ],
      "metadata": {
        "id": "32_YRs7RZ_Bw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSh1ewj3XWJ6",
        "outputId": "1028ab34-681b-4dc1-ab07-989304ec2ffa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              Common.Name       Date   Time  n.observers     County  Sub.cell  \\\n",
            "0              Asian Koel  7/16/2015  16:30          2.0  Alappuzha  [51,2,2]   \n",
            "1  Black-rumped Flameback  7/16/2015  16:30          2.0  Alappuzha  [51,2,2]   \n",
            "2            Black Drongo  7/16/2015  16:30          2.0  Alappuzha  [51,2,2]   \n",
            "3           Brahminy Kite  7/16/2015  16:30          2.0  Alappuzha  [51,2,2]   \n",
            "4             Common Myna  7/16/2015  16:30          2.0  Alappuzha  [51,2,2]   \n",
            "\n",
            "  Season  DEM       Cell.ID List.ID  \n",
            "0    Wet  5.0  [76.28,9.84]  List.1  \n",
            "1    Wet  5.0  [76.28,9.84]  List.1  \n",
            "2    Wet  5.0  [76.28,9.84]  List.1  \n",
            "3    Wet  5.0  [76.28,9.84]  List.1  \n",
            "4    Wet  5.0  [76.28,9.84]  List.1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading dataset"
      ],
      "metadata": {
        "id": "zVNYJKC6aKg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/kba_data.csv')"
      ],
      "metadata": {
        "id": "qEERU_bUZ6Xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking data types"
      ],
      "metadata": {
        "id": "N_Q8ztyfaPRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.info())"
      ],
      "metadata": {
        "id": "ykPO8ydYaOoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking first 5 row"
      ],
      "metadata": {
        "id": "9k4o7WDVadGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.head())"
      ],
      "metadata": {
        "id": "UOLirYeEaWFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform One-Hot Encoding for categorical variables: \"Season\"\n"
      ],
      "metadata": {
        "id": "rvcRtAMpdR-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_encoded = pd.get_dummies(data, columns=[\"Season\"], drop_first=True)\n"
      ],
      "metadata": {
        "id": "s9xqtN76dSZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separate independent and dependent variables"
      ],
      "metadata": {
        "id": "cAPrglpgdfaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_encoded.drop(\"DEM\", axis=1)  # Assuming \"DEM\" is the target variable\n",
        "y = df_encoded[\"DEM\"]"
      ],
      "metadata": {
        "id": "4zNqinlldf1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting train and test sets"
      ],
      "metadata": {
        "id": "wyRfrD-AcXj2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OB7DPMoVVXut"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check for missing values"
      ],
      "metadata": {
        "id": "VO6xbQl1eTbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values = data.isnull().sum()\n",
        "print(\"Missing Values:\\n\", missing_values)"
      ],
      "metadata": {
        "id": "UMD3wWlferxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drop rows with missing values"
      ],
      "metadata": {
        "id": "GhdfwRqMe6jx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "js11LUigfAgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalize numerical features if necessary.For example, using StandardScaler"
      ],
      "metadata": {
        "id": "dvg9FIyqfEN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "data[['n.observers']] = scaler.fit_transform(data[['n.observers']])\n"
      ],
      "metadata": {
        "id": "Z4MAebRqfEoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWQLh1Qoi7VZ",
        "outputId": "027c6338-1389-4980-82c5-edd2565f4436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Common.Name', 'Date', 'Time', 'n.observers', 'County', 'Sub.cell',\n",
            "       'Season', 'DEM', 'Cell.ID', 'List.ID', 'Season_new'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One Hot Encoder"
      ],
      "metadata": {
        "id": "bTNC-C8Ff-8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Assuming 'df' is your DataFrame with columns: Common.Name, Date, Time, n.observers, County, Sub.cell, Season, DEM, Cell.ID, List.ID, Season_new\n",
        "\n",
        "# Convert specified columns to category type\n",
        "columns_to_convert = ['Common.Name', 'Date', 'Time', 'County', 'Sub.cell', 'Season', 'DEM', 'Cell.ID', 'List.ID']\n",
        "for col in columns_to_convert:\n",
        "    data[col] = data[col].astype('category')\n",
        "\n",
        "# Assign numerical values to the categorical columns and store them in new columns\n",
        "for col in columns_to_convert:\n",
        "    data[col+'_new'] = data[col].cat.codes\n",
        "\n",
        "# Create an instance of OneHotEncoder\n",
        "encoder = OneHotEncoder()\n",
        "\n",
        "# Perform One-Hot Encoding on the specified categorical columns and store the encoded data in a DataFrame\n",
        "enc_data = pd.DataFrame(encoder.fit_transform(data[[col+'_new' for col in columns_to_convert]]).toarray(), columns=encoder.get_feature_names_out(columns_to_convert))\n",
        "\n",
        "# Merge the encoded DataFrame with the original DataFrame\n",
        "new_df = pd.concat([data, enc_data], axis=1)\n",
        "\n",
        "# Drop the original categorical columns and the numerical encoding columns\n",
        "new_df.drop(columns_to_convert + [col+'_new' for col in columns_to_convert], axis=1, inplace=True)\n",
        "\n",
        "print(new_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "kAS3E6XrjcL0",
        "outputId": "de10f029-814d-4371-b1a9-9d32c043a8b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "input_features is not equal to feature_names_in_",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-98395349a38f>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Perform One-Hot Encoding on the specified categorical columns and store the encoded data in a DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0menc_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_new'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns_to_convert\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns_to_convert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Merge the encoded DataFrame with the original DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mget_feature_names_out\u001b[0;34m(self, input_features)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         \"\"\"\n\u001b[1;32m   1101\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m         \u001b[0minput_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_feature_names_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         cats = [\n\u001b[1;32m   1104\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_transformed_categories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_check_feature_names_in\u001b[0;34m(estimator, input_features, generate_names)\u001b[0m\n\u001b[1;32m   1952\u001b[0m             \u001b[0mfeature_names_in_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1953\u001b[0m         ):\n\u001b[0;32m-> 1954\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"input_features is not equal to feature_names_in_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1956\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features_in_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_features\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: input_features is not equal to feature_names_in_"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AlfaKl8Djxmf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ln6MEkO0jxDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-it2r2Kejx5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_encoded = pd.get_dummies(df, columns=[\"DEM\", \"season\", \"county\"], drop_first=True)"
      ],
      "metadata": {
        "id": "lKTApCsEjw2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_encoded.drop(\"charges\", axis=1)\n",
        "y = df_encoded[\"charges\"]\n",
        "\n",
        "# Split X and y into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Scale the features\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "zdGLdXxbizFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression Models"
      ],
      "metadata": {
        "id": "cILmymQEeTXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "\n",
        "# Select regression models\n",
        "linear_regression_model = LinearRegression()\n",
        "ridge_regression_model = Ridge()\n",
        "decision_tree_regression_model = DecisionTreeRegressor()\n",
        "random_forest_regression_model = RandomForestRegressor()\n",
        "gradient_boosting_regression_model = GradientBoostingRegressor()\n",
        "\n",
        "# Train regression models\n",
        "linear_regression_model.fit(X_train_scaled, y_train)\n",
        "ridge_regression_model.fit(X_train_scaled, y_train)\n",
        "decision_tree_regression_model.fit(X_train_scaled, y_train)\n",
        "random_forest_regression_model.fit(X_train_scaled, y_train)\n",
        "gradient_boosting_regression_model.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "a0oZ53ndeS_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oevLGl9nfrWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Predict on the training set\n",
        "y_train_pred_linear = linear_regression_model.predict(X_train_scaled)\n",
        "y_train_pred_ridge = ridge_regression_model.predict(X_train_scaled)\n",
        "y_train_pred_decision_tree = decision_tree_regression_model.predict(X_train_scaled)\n",
        "y_train_pred_random_forest = random_forest_regression_model.predict(X_train_scaled)\n",
        "y_train_pred_gradient_boosting = gradient_boosting_regression_model.predict(X_train_scaled)\n",
        "\n",
        "# Calculate MSE for training predictions\n",
        "mse_linear = mean_squared_error(y_train, y_train_pred_linear)\n",
        "mse_ridge = mean_squared_error(y_train, y_train_pred_ridge)\n",
        "mse_decision_tree = mean_squared_error(y_train, y_train_pred_decision_tree)\n",
        "mse_random_forest = mean_squared_error(y_train, y_train_pred_random_forest)\n",
        "mse_gradient_boosting = mean_squared_error(y_train, y_train_pred_gradient_boosting)\n",
        "\n",
        "# Print the MSE for each model\n",
        "print(\"MSE - Linear Regression:\", mse_linear)\n",
        "print(\"MSE - Ridge Regression:\", mse_ridge)\n",
        "print(\"MSE - Decision Tree Regression:\", mse_decision_tree)\n",
        "print(\"MSE - Random Forest Regression:\", mse_random_forest)\n",
        "print(\"MSE - Gradient Boosting Regression:\", mse_gradient_boosting)"
      ],
      "metadata": {
        "id": "y0A11H1-ftIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import make_scorer, mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "\n",
        "# Define regression models\n",
        "models = [\n",
        "    LinearRegression(),\n",
        "    Ridge(),\n",
        "    DecisionTreeRegressor(),\n",
        "    RandomForestRegressor(),\n",
        "    GradientBoostingRegressor()\n",
        "]\n",
        "\n",
        "# Create a custom scorer for cross-validation (negative mean squared error)\n",
        "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
        "\n",
        "# Perform cross-validation and evaluate models\n",
        "results = {}\n",
        "for model in models:\n",
        "    scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring=scorer)\n",
        "    results[str(model)] = -scores.mean()  # Convert negative MSE back to positive for better interpretation\n",
        "\n",
        "# Print the cross-validation results\n",
        "for model, score in results.items():\n",
        "    print(f\"{model}: Mean MSE = {score:.2f}\")\n",
        "\n",
        "# Choose the best performing model\n",
        "best_model = min(results, key=results.get)\n",
        "print(\"\\nBest performing model:\", best_model)\n"
      ],
      "metadata": {
        "id": "l-HETcG2ldZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid to search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],   # Number of boosting stages\n",
        "    'learning_rate': [0.01, 0.1, 0.2], # Step size at each iteration\n",
        "    'max_depth': [3, 4, 5]             # Maximum depth of individual regression estimators\n",
        "}\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search = GridSearchCV(GradientBoostingRegressor(), param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Perform the grid search\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best negative MSE:\", -grid_search.best_score_)\n"
      ],
      "metadata": {
        "id": "Hu3unYlClk1X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}